{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Neural network regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "from scipy.stats import multivariate_normal as mv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import *\n",
    "from numpy import *\n",
    "# q1\n",
    "# create the target distribution\n",
    "# sample at .1 interval\n",
    "tmp = arange(-2, 2, .1)\n",
    "x, y = meshgrid(tmp, tmp)\n",
    "\n",
    "mu = [0,0]\n",
    "sigma = eye(len(mu)) * 2/5\n",
    "dist = mv(mu, sigma)\n",
    "\n",
    "X = vstack((x.flatten(), y.flatten())).T\n",
    "Y = dist.pdf(X).reshape(x.shape)  * 3\n",
    "targets = array(Y.flatten(), ndmin = 2).T\n",
    "\n",
    "fig, ax = subplots(1, 1, subplot_kw = {'projection': '3d'})\n",
    "ax.plot_surface(x, y, targets.reshape(x.shape))\n",
    "ax.set_xlabel('$x_1$', labelpad = 20)\n",
    "ax.set_ylabel('$x_2$', labelpad = 20)\n",
    "ax.set_zlabel('pdf', labelpad =20)\n",
    "sb.set_context('poster')\n",
    "sb.set_style('white')\n",
    "fig.suptitle('Target distribution')\n",
    "savefig('../Figures/2.1.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class mlp(object):\n",
    "    '''\n",
    "    Multi-layered-pereptron\n",
    "    K = output nodes\n",
    "    M = hidden nodes\n",
    "    Assumes the input data X is samples x feature dimension\n",
    "    Returns:\n",
    "        prediction and error\n",
    "    '''\n",
    "    def __init__(self, X, t,\\\n",
    "                 eta = 1e-1,\\\n",
    "                 gamma = .0,\\\n",
    "                 M = 8,\\\n",
    "                 K = 1):\n",
    "        # learning rate / momentum rate\n",
    "        self.eta        = eta\n",
    "        self.gamma      = gamma\n",
    "        # Layer dimensions; input, hidden, output\n",
    "        self.D          = D =  X.shape[1] + 1\n",
    "        self.M          = M\n",
    "        self.K          = K\n",
    "        # add bias node to input\n",
    "        self.X          = hstack( (X, ones(( X.shape[0], 1 ) ) ) )\n",
    "        self.targets    = t\n",
    "        # weights; hidden and output\n",
    "        wh              = random.rand(D, M) - 1/2\n",
    "        wo              = random.rand(M, K) - 1/2\n",
    "\n",
    "        self.layers     = [wh, wo]\n",
    "        # activation functions:\n",
    "        self.func       = lambda x: tanh(x)\n",
    "        self.dfunc      = lambda x: 1 - x**2\n",
    "        \n",
    "\n",
    "    def forwardSingle(self, xi):\n",
    "        ''' Performs a single forward pass in the network'''\n",
    "        layerOutputs = [ []  for j in self.layers ]\n",
    "        #forward pass\n",
    "        a = xi.dot(self.layers[0])\n",
    "        z = self.func(a)\n",
    "        y = z.dot(self.layers[1])\n",
    "        \n",
    "        # save output\n",
    "        layerOutputs[0].append(z);\n",
    "        layerOutputs[1].append(y)\n",
    "        return layerOutputs\n",
    "    \n",
    "    def backwardsSingle(self, ti, xi, forwardPass):\n",
    "        '''Backprop + update of weights'''\n",
    "        # prediction error\n",
    "        dk = forwardPass[-1][0] - ti\n",
    "        squaredError = dk**2\n",
    "        # compute hidden activation; note elementwise product!!\n",
    "        dj = \\\n",
    "        self.dfunc(forwardPass[0][0]) * (dk.dot(self.layers[-1].T))\n",
    "\n",
    "        # update the weights\n",
    "        E1 = forwardPass[0][0].T.dot(dk)\n",
    "        E2 = xi.T.dot(dj)\n",
    "        \n",
    "        # update weights of layers\n",
    "        self.layers[-1] -= \\\n",
    "        self.eta * E1 + self.gamma * self.layers[-1]\n",
    "        \n",
    "        self.layers[0]  -= \\\n",
    "        self.eta * E2 + self.gamma * self.layers[0]\n",
    "        return squaredError\n",
    "    \n",
    "    def train(self, num, plotProg = (False,)):\n",
    "        #set up figure\n",
    "        if plotProg[0]:\n",
    "            fig, ax = subplots(subplot_kw = {'projection':'3d'})\n",
    "\n",
    "            \n",
    "        num   = int(num) # for scientific notation\n",
    "        SSE   = zeros(num) # sum squared error\n",
    "        preds = zeros((num, len(self.targets))) # predictions per run\n",
    "        for iter in range(num):\n",
    "            error = 0 # sum squared error\n",
    "            for idx, (ti, xi) in enumerate(zip(self.targets, self.X)):\n",
    "                xi = array(xi, ndmin = 2)\n",
    "\n",
    "                forwardPass = self.forwardSingle(xi)\n",
    "                error += self.backwardsSingle(ti, xi, forwardPass)\n",
    "                preds[iter, idx] = forwardPass[-1][0]         \n",
    "            # plot progress\n",
    "            if plotProg[0]:\n",
    "                if not iter % plotProg[1]:\n",
    "                    x, y = plotProg[2]\n",
    "                    ax.cla() # ugly workaround \n",
    "                    ax.plot_surface(x, y, preds[iter, :].reshape(x.shape))\n",
    "                    ax.set_xlabel('$x_1$', labelpad = 20)\n",
    "                    ax.set_ylabel('$x_2$', labelpad = 20)\n",
    "                    ax.set_zlabel('pdf', labelpad =20)\n",
    "                    ax.set_title('Cycle = {0}'.format( iter ))\n",
    "                    pause(1e-10)\n",
    "            SSE[iter] = .5 * error\n",
    "        return SSE, preds\n",
    "\n",
    "# perform a single forward pass and show the results\n",
    "model = mlp(X, targets)\n",
    "# perform a single pass\n",
    "preds = array([\\\n",
    "              model.forwardSingle(\\\n",
    "              array(hstack( ( xi, 1) ),\\\n",
    "                    ndmin = 2))[-1]\\\n",
    "              for xi in X]).flatten()\n",
    "# plot the results\n",
    "fig, ax = subplots(subplot_kw  = {'projection': '3d'})\n",
    "ax.scatter(x, y, preds.reshape(x.shape))\n",
    "ax.set_xlabel('$x_1$', labelpad = 20)\n",
    "ax.set_ylabel('$x_2$', labelpad = 20)\n",
    "ax.set_zlabel('pdf', labelpad =20)\n",
    "ax.set_title('Network output without training')\n",
    "sb.set_context('poster')\n",
    "savefig('../Figures/2.2.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run at atleast 500 cycles\n",
    "num = int(5e2) + 1\n",
    "model = mlp(X, targets)\n",
    "# train the model\n",
    "SSE, preds = model.train(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot every 250 cycles\n",
    "cycleRange = arange(0,  num, num // 5)\n",
    "\n",
    "# use at most 3 rows\n",
    "nRows = 3\n",
    "nCols = int(ceil(len(cycleRange)/nRows))\n",
    "nCols = 2\n",
    "\n",
    "fig, axes  = subplots(nRows,\\\n",
    "                      nCols,\\\n",
    "                      subplot_kw = {'projection': '3d'})\n",
    "\n",
    "idx = random.permutation(len(targets))\n",
    "for axi, i in enumerate(cycleRange):\n",
    "    ax = axes.flatten()[axi]\n",
    "    ax.plot_surface(\\\n",
    "                    x,\\\n",
    "                    y,\\\n",
    "                    preds[i, :].reshape(x.shape),\\\n",
    "                    cstride = 10,\\\n",
    "                    rstride = 10)\n",
    "    # formatting of plot\n",
    "    ax.set_xlabel('$x_1$', labelpad = 20)\n",
    "    ax.set_ylabel('$x_2$', labelpad = 20)\n",
    "    ax.set_zlabel('pdf', labelpad =20)\n",
    "    ax.set_title('cycles = {0}'.format(i))\n",
    "    sb.set_style('white')\n",
    "fig.delaxes(axes.flatten()[-1])\n",
    "fig.suptitle('Output of MLP as a function of complete cycles')\n",
    "subplots_adjust(top=0.8)\n",
    "# fig.tight_layout()\n",
    "# savefig('../Figures/2.3.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle the indices\n",
    "idx = random.permutation(len(targets))\n",
    "\n",
    "# shuffle the data\n",
    "shuffleX = X[idx,:]\n",
    "# shuffle the targets as well (same indices)\n",
    "shuffleTargets = targets[idx, :]\n",
    "shuffleModel = mlp(shuffleX, shuffleTargets)\n",
    "shuffleSSE, shufflePreds = shuffleModel.train(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the results: compare with error from [3]\n",
    "fig, ax = subplots()\n",
    "ax.plot( range(num),SSE,\\\n",
    "        range(num), shuffleSSE)\n",
    "\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Sum squared error')\n",
    "ax.legend(['shuffled','non-shuffled'],loc = 0)\n",
    "fig.suptitle('Training error')\n",
    "# savefig('../Figures/2.4.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax  = subplots(subplot_kw = {'projection': '3d'})\n",
    "ax.plot_surface(x, y, shufflePreds[0, argsort(idx)].reshape(x.shape))\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Since the grid is linearly spaced, this will mean that nearby points will yield the same gradient in the error. This 'local' correlation will yield that the algorithm will change the weights in similar direction for a while, hence shuffling the data removes this 'local' correlation structure, yielding more likely to move in the different directions, constraining the algorithm, yielding faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "X, Y, target = list(np.loadtxt('../Data/a017_NNpdfGaussMix.txt').T)\n",
    "tmp = int(np.sqrt(target.shape[0]))\n",
    "# convert in shape for it to be plottable \n",
    "x = X.reshape(tmp,tmp)\n",
    "\n",
    "y = Y.reshape(tmp, tmp)\n",
    "# stack to create input data\n",
    "X = np.vstack((X,Y)).T\n",
    "\n",
    "# target vector\n",
    "target = np.array(target, ndmin = 2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the target distribution\n",
    "fig, ax = subplots(1,1, subplot_kw = {'projection': '3d'})\n",
    "ax.plot_surface(x, y, target.reshape(tmp, tmp))\n",
    "ax.set_xlabel('$x_1$', labelpad = 20)\n",
    "ax.set_ylabel('$x_2$', labelpad = 20)\n",
    "ax.set_zlabel('pdf', labelpad = 20)\n",
    "fig.suptitle('Target distribution')\n",
    "# savefig('../Figures/2.5.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#randomly permute indices\n",
    "idx = np.random.permutation(range(len(target)))\n",
    "# keep track of the changes\n",
    "shuffX = X[idx,:]; shuffTarget = target[idx]\n",
    "# run mlp with eta = .01\n",
    "model = mlp(shuffX, shuffTarget,\\\n",
    "                    eta = 1e-2,\\\n",
    "                    M = 40)\n",
    "\n",
    "# run for 2000 complete cylcles\n",
    "num = int(2e3)\n",
    "errors, preds = model.train(num = num)\n",
    "# map back to original space\n",
    "orgIdx = argsort(idx)\n",
    "# get final prediction\n",
    "finalPred = preds[-1, orgIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the final prediction and the target distribution\n",
    "fig, ax = subplots(subplot_kw = {'projection': '3d'})\n",
    "ax.scatter(x, y, target.reshape(x.shape), label = 'target')\n",
    "ax.scatter(x, y, finalPred.reshape(x.shape), label = 'estimation')\n",
    "ax.set_xlabel('$x_1$', labelpad = 20)\n",
    "ax.set_ylabel('$x_2$', labelpad = 20)\n",
    "ax.set_zlabel('pdf', labelpad   = 20)\n",
    "ax.legend(loc = 0)\n",
    "fig.suptitle('Final prediction after {0} cycles'.format(num))\n",
    "savefig('../Figures/2.61.png')\n",
    "\n",
    "fig, ax = subplots()\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Sum squared error')\n",
    "ax.set_title('Training error')\n",
    "savefig('../Figures/2.62.png')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might improve the performance by adding more hidden nodes to the network. The hidden nodes essentially represent the degrees of freedom in the model. Increasing the number of hidden nodes might increase the fit on a trainingset, however it will also increase the modelling noise (i.e overfitting). Improvements might also be found in presenting the inputs / targets in a different feature space. Multi-layered perceptrons are notorious for being sensitive to how the data is represented. Another might be instead of taking a global learning rate, is make it adaptive (see conjugate gradient descent, momentum etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using python hence netlab toolbox is not available to us. We opted for the neurolab toolbox which has a conjugate gradient method. The same parameters were used as in 7 to improve comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Warning: CG iterations didn't converge.  The Hessian is not positive definite.\n",
      "         Current function value: 2.973533\n",
      "         Iterations: 25\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 6893\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "import neurolab as nl\n",
    "from scipy.optimize import fmin_ncg\n",
    "# inputs and targets\n",
    "inp = shuffX\n",
    "tar = shuffTarget\n",
    "\n",
    "# specify same network structure as we have\n",
    "# i.e. M = 40, D = 3, K = 1\n",
    "# this function takes min/max of input space\n",
    "\n",
    "# Specify activation functions;\n",
    "# input - to hidden  is hyperbolic tangent\n",
    "# hidden- to out is linear\n",
    "tranfs = [nl.trans.TanSig(), nl.trans.PureLin()]\n",
    "net = nl.net.newff(\\\n",
    "                   [ [np.min(X), np.max(X)] ] * inp.shape[1],\\\n",
    "                   [40, 1],\\\n",
    "                   transf= tranfs,\\\n",
    "                  )\n",
    "# use conjugate gradient as method\n",
    "net.trainf = nl.train.train_ncg # conjugate gradient\n",
    "# net.trainf = fmin_ncg\n",
    "net.errorf  = nl.error.SSE()    # same as above\n",
    "\n",
    "# init weight matrix between -.5,.5\n",
    "for l in net.layers:\n",
    "    l.initf = nl.init.InitRand([-.5, .5], 'wb')\n",
    "net.init()\n",
    "\n",
    "# Train network; show output ever 500 iterations\n",
    "errorNL = net.train(inp, tar, epochs= num, show = 500)\n",
    "\n",
    "# Simulate network\n",
    "outNL = net.sim(inp)\n",
    "# sort back to original indices\n",
    "outNL = outNL[argsort(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final SSE neruolab :\n",
      " 2.9735334308131867\n",
      "final SSE our alogorithm:\n",
      "1.3135305939589321\n"
     ]
    }
   ],
   "source": [
    "# plot the performance versus our algorithm\n",
    "# plot the final prediction and the target distribution\n",
    "fig, ax = subplots(subplot_kw = {'projection': '3d'})\n",
    "ax.scatter(x, y,\\\n",
    "           target.reshape(x.shape), label = 'target')\n",
    "ax.scatter(x, y,\\\n",
    "           outNL.reshape(x.shape), label = 'estimation')\n",
    "\n",
    "ax.set_xlabel('$x_1$', labelpad = 20)\n",
    "ax.set_ylabel('$x_2$', labelpad = 20)\n",
    "ax.set_zlabel('pdf', labelpad = 20)\n",
    "ax.legend(loc = 0)\n",
    "fig.suptitle('Neurolab prediction')\n",
    "print(\\\n",
    "'final SSE neruolab :\\n {0}'\\\n",
    ".format(errorNL[-1]))\n",
    "print(\\\n",
    "'final SSE our alogorithm:\\n{0}'.format(errors[-1]))\n",
    "# savefig('../Figures/2.7.png')\n",
    "\n",
    "# plot training errors\n",
    "fig, ax = subplots()\n",
    "ax.plot(errorNL, label = 'conjugate gradient')\n",
    "ax.plot(errors, label = 'our MLP')\n",
    "ax.set_xlabel('Training cycles')\n",
    "ax.set_ylabel('Sum squared error')\n",
    "fig.suptitle('Conjugate gradient convergence')\n",
    "ax.legend(loc = 0)\n",
    "# savefig('../Figures/2.71.png')\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errorNL)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
