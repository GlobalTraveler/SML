\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Exercise 2 - Neural network regression}{1}{section.1}}
\newlabel{exercise-2---neural-network-regression}{{1}{1}{Exercise 2 - Neural network regression}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}1}{1}{section.2}}
\newlabel{section}{{2}{1}{1}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Target distribution for the MLP in the later exercises}}{2}{figure.1}}
\newlabel{fig:2.1}{{1}{2}{Target distribution for the MLP in the later exercises}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}2}{2}{subsection.2.1}}
\newlabel{section}{{2.1}{2}{2}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results for 2.2; output of the network without any training. The output is a 2D plane since the weight have been sampled uniformly.}}{5}{figure.2}}
\newlabel{fig:2.2}{{2}{5}{Results for 2.2; output of the network without any training. The output is a 2D plane since the weight have been sampled uniformly}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}3}{5}{section.3}}
\newlabel{section}{{3}{5}{3}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This figure shows the output of the network as a function of training cycles. Note that cycle = 0 is after 1 full pass of the network. Python starts indexing from 0. This if for the non-shuffled data.}}{7}{figure.3}}
\newlabel{fig:2.3}{{3}{7}{This figure shows the output of the network as a function of training cycles. Note that cycle = 0 is after 1 full pass of the network. Python starts indexing from 0. This if for the non-shuffled data}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}4}{7}{section.4}}
\newlabel{section}{{4}{7}{4}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The sum squared error as a function of training cycles. Note a full cycle is after seeing all the training samples, weights are updated after seeing a datapoint. Noticeably, we see that the shuffled data is faster in converging, for explanation see text.}}{8}{figure.4}}
\newlabel{fig:2.4}{{4}{8}{The sum squared error as a function of training cycles. Note a full cycle is after seeing all the training samples, weights are updated after seeing a datapoint. Noticeably, we see that the shuffled data is faster in converging, for explanation see text}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}5}{8}{section.5}}
\newlabel{section}{{5}{8}{5}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Target distribution loaded from the provided data files}}{9}{figure.5}}
\newlabel{fig:2.5}{{5}{9}{Target distribution loaded from the provided data files}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}6}{9}{section.6}}
\newlabel{section}{{6}{9}{6}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces In green we gave the prediction generated from our network, for parameters please see the code block in this exercise. The performance of training is shown in figure \ref  {fig:2.62}. Note that the little hump is not captured, but the sum squared error remains low.}}{11}{figure.6}}
\newlabel{fig:2.61}{{6}{11}{In green we gave the prediction generated from our network, for parameters please see the code block in this exercise. The performance of training is shown in figure \ref {fig:2.62}. Note that the little hump is not captured, but the sum squared error remains low}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Sum squared error as a function of completed training cycles.}}{11}{figure.7}}
\newlabel{fig:2.62}{{7}{11}{Sum squared error as a function of completed training cycles}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}7}{12}{section.7}}
\newlabel{section}{{7}{12}{7}{section.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Output of conjugate gradient from neurolab toolbox.}}{14}{figure.8}}
\newlabel{fig:2.7}{{8}{14}{Output of conjugate gradient from neurolab toolbox}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training error of the neurolab implementation versus ours. Note that the rate of convergence if faster than our implementation. However, it did halt after 25 iterations due to a failure to converge. It uses the methods from scipy.}}{15}{figure.9}}
\newlabel{fig:2.71}{{9}{15}{Training error of the neurolab implementation versus ours. Note that the rate of convergence if faster than our implementation. However, it did halt after 25 iterations due to a failure to converge. It uses the methods from scipy}{figure.9}{}}
