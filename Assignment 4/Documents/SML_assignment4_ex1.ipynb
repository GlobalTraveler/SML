{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Gaussian Processes\n",
    "## 1\n",
    "Not much to say it does what it says on the tin; implements equation (3) of the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from numpy import *\n",
    "\n",
    "close('all')\n",
    "#  q1\n",
    "def kernel(x1, x2, theta):\n",
    "    \"\"\"\n",
    "    Code for equation 3\n",
    "    \"\"\"\n",
    "    sigma = np.zeros((len(x1), len(x2)))\n",
    "    for idx, i in enumerate(x1):\n",
    "        for jdx, j in enumerate(x2):\n",
    "            sigma[ idx, jdx ] = \\\n",
    "                theta[ 0 ] *\\\n",
    "                np.exp(- .5 * theta[ 1 ] * np.sqrt((i - j)**2)) + \\\n",
    "                theta[ 2 ] + theta[ 3 ] * i * j\n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "Define theta and the linearly spaced points and use the kernel function from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#q2\n",
    "theta = ones(4)\n",
    "N = 101\n",
    "x = linspace(-1,1, N)\n",
    "K = kernel(x, x, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gramm matrix would be 101x101, i.e. showing the variance of the dataspace. In order to show that a matrix is semipositive definite we need to show that all eigenvalues are non-negative, $\\lambda_i \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of K (101, 101)\n",
      "are all eigenvalues non-negative? True\n"
     ]
    }
   ],
   "source": [
    "print('Shape of K', K.shape)\n",
    "eigenValues, eigenVectors = linalg.eig(K)\n",
    "print(r'are all eigenvalues non-negative?', all(eigenValues >= 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# q4\n",
    "# import multivariate normal object\n",
    "from scipy.stats import multivariate_normal as mv\n",
    "# define mu / sigma\n",
    "mu = zeros(len(x))\n",
    "# prior object\n",
    "prior = mv(mu, K)\n",
    "# sample from prior\n",
    "samples = prior.rvs(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show samples from the prior\n",
    "fig, ax = subplots(1, 1)\n",
    "for i in samples:\n",
    "    ax.plot(x, i)\n",
    "savefig('../Figures/ex1q5')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('5 randomly drawn functions from prior')\n",
    "savefig('../Figures/1.4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# q5\n",
    "thetas = np.array([ [ 1, 4, 0, 0 ],\\\n",
    "                   [ 9, 4, 0, 0 ],\\\n",
    "                   [ 1, 64, 0, 0 ],\n",
    "                 [ 1, 0.25, 0, 0 ],\\\n",
    "                   [ 1, 4, 10, 0 ],\\\n",
    "                   [ 1, 4, 0, 5 ] ])\n",
    "\n",
    "# we want at most 3 rows\n",
    "nRows = 3\n",
    "# get the the number of columns\n",
    "nCols = thetas.shape[ 0 ] // nRows\n",
    "\n",
    "# plot the prior for different theta\n",
    "fig, ax = subplots(nrows=nRows, ncols=nCols, sharex='all')\n",
    "for idx, thetai in enumerate(thetas):\n",
    "    # compute the Gramm matrix\n",
    "    K = kernel(x, x, thetai)\n",
    "    # generate prior object\n",
    "    prior = mv(mu, K, allow_singular=1)\n",
    "    # sample from the prior\n",
    "    samples = prior.rvs(5)\n",
    "    for i in samples:\n",
    "        ax.flatten()[ idx ].plot(x, i)\n",
    "    ax.flatten() [ idx ].set_title(r'$\\theta$ = {0}'.format(thetai))\n",
    "    ax.flatten() [ idx ].set_xlabel('x')\n",
    "    ax.flatten() [ idx ].set_ylabel('y')\n",
    "fig.suptitle(r'Samples from prior with different $\\theta$ ')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "savefig('../Figures/1.5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus explanation\n",
    "From the left column top two plots, and middle right we see that $\\theta[1]$ regulates the amount of 'jitter' in the random process. From (3) we see that $\\theta[1]$ weighs the euclidean distance between $x, x'$. Comparing the top row plots, we see that the non-linearity of the random process is controlled by increasing $\\theta[0]$, i.e. (global) curvature of the lines. The off-set of the random process is controlled by $\\theta[2]$ (trivially) this can be seen in comparing the bottom and top plot in the left column. Finally, $\\theta[3]$ controls the amount of linearity of change in the random process. This can be seen in bottom right plot, compared to the top row of plots. So in all we can summarize it as:\n",
    "\n",
    "\n",
    "\\begin{enumerate}\n",
    "  \\item - Given the kernel function in (3):\n",
    "  \\begin{itemize}\n",
    "  \\item  $\\theta[0]$ controls the non-linear component of the random process\n",
    "\n",
    "  \\item  $\\theta[1]$ controls the 'jitterness' of the random process, i.e. weigh the euclidean distance in the random process\n",
    "\n",
    "  \\item $\\theta[2]$ controls the mean off set in the random process\n",
    "\n",
    "  \\item $\\theta[3]$ controls the linear component of change of the random process\n",
    "  \\end{itemize}\n",
    "\\end{enumerate}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing C we will use equation 6.62 from Bisschop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\n",
      "  [[ 3.25        1.60468809  1.52032005  1.86873075]\n",
      " [ 1.60468809  3.04        2.01122942  1.84070798]\n",
      " [ 1.52032005  2.01122942  3.09        1.78873075]\n",
      " [ 1.86873075  1.84070798  1.78873075  3.01      ]]\n"
     ]
    }
   ],
   "source": [
    "# q6\n",
    "# define training set; inputs, targets\n",
    "xTrain = array([[-.5, .2, .3 , -.1]]).T\n",
    "tTrain = array([[.5, -1, 3, -2.5]]).T\n",
    "KTrain = kernel(xTrain, xTrain, theta)\n",
    "beta   = 1\n",
    "C      = KTrain + 1/beta * eye(len(xTrain))\n",
    "print('C:\\n ',C)\n",
    "\n",
    "# show the training points\n",
    "fig, ax = subplots()\n",
    "ax.scatter(xTrain, tTrain)\n",
    "ax.set_title('Scatter plot of training points')\n",
    "ax.set_ylabel('Targets')\n",
    "ax.set_xlabel('Input')\n",
    "savefig('../Figures/1.6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the $\\mu$ at x = 0, we will to compute the Gramm matrix \n",
    "using the training points and the new point (see Bisschop p.307); i.e. we compute\n",
    "    *K* = K($\\boldsymbol{X_n}, \\boldsymbol{X_{N+1}}$), where $\\boldsymbol{X_n}$ is the set\n",
    "    $i = 1,...,N$ and $X_{N+1} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu at x = [0]:\n",
      "[[-0.20721389]]\n",
      "sigma^2 at x = [0]:\n",
      "[[ 1.32520089]]\n"
     ]
    }
   ],
   "source": [
    "xNew       = array([[0]])\n",
    "xTrainNew  = vstack((xTrain, xNew))\n",
    "c          = kernel(xNew, xNew, theta) + 1/beta\n",
    "k          = kernel(xTrain, xNew, theta)\n",
    "invC       = linalg.inv(C)\n",
    "# Bisschop 6.66\n",
    "mu_new     = k.T.dot(invC).dot(tTrain)\n",
    "# Bisschop 6.67\n",
    "sigma_new  = c - k.T.dot(invC).dot(k)\n",
    "print('mu at x = {0}:\\n{1}'.format(xNew[0], mu_new) )\n",
    "print('sigma^2 at x = {0}:\\n{1}'.format(xNew[0], sigma_new) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of $p(t \\mid \\boldsymbol(t)))$ will not go to zero as $x \\rightarrow \\pm \\infty$. If $x$ approaches $\\pm \\infty$ the gaussian part of (3) will go to zero but the linear part ($x^T x'$) will blow up to $\\pm \\infty$ (depending on the sign of $x'$. Thus to make the kernel function to go to zero we would need to suppress $x^T x'$ by setting $\\theta[3] = \\theta[2] =  0$ (the bias, $\\theta[2]$ will trivially cause non-zero if set to anything else than zero).\n",
    "\n",
    "As an example, we take a large value for x and print the mean by setting $\\theta[2] = \\theta[3] = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu at x  = [  1.00000000e+30]:\n",
      "[[ 0.]]\n",
      "and theta:\n",
      " [ 1.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# take a very large number\n",
    "xNew       = array([[1e30]])\n",
    "xTrainNew  = vstack((xTrain, xNew))\n",
    "c          = kernel(xNew, xNew, theta) + 1/beta\n",
    "thetaEdit  = theta.copy()\n",
    "thetaEdit[-2:] = 0\n",
    "k          = kernel(xTrain, xNew, thetaEdit)\n",
    "invC       = linalg.inv(C)\n",
    "# Bisschop 6.66\n",
    "mu_new     = k.T.dot(invC).dot(tTrain)\n",
    "# Bisschop 6.67\n",
    "sigma_new  = c - k.T.dot(invC).dot(k)\n",
    "print('mu at x  = {0}:\\n{1}\\nand theta:\\n {2}'\\\n",
    "      .format(xNew[0], mu_new, thetaEdit) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
